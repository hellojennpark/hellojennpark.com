---
title: "From Docker to Kubernetes: A 3-Year Journey"
description: "A candid walkthrough of how I experienced deployment strategies evolving across two companies-from Docker containerization to full Kubernetes orchestration. This post shares real problems, practical solutions, and honest lessons learned from supporting deployments for 300+ engineers, including the good, the bad, and the learning moments."
tags: [
  "devops",
  "docker",
  "kubernetes",
  "deployment",
  "microservices",
  "infrastructure",
  "experience-report",
  "containers",
  "orchestration",
  "ci-cd"
]
date: "2024-08-16"
---

> I hope this journey inspires you to embrace the challenges of evolving deployment strategies, learning from each pain point and solution. If I could go back, I’d document every step more meticulously-detailed notes would’ve made this post richer and helped my team troubleshoot faster. My biggest regret is not starting this habit earlier, but moving forward, I’m committed to consistent documentation.

## The Journey Overview
This isn't a story about chasing the latest technology trends-it's about learning real solutions to real problems, understanding different deployment approaches, and discovering that the best technology is the one that fits your current situation.

This journey spanned two companies and various service types: API servers, web applications, and socket servers. Each service type presented unique deployment challenges, but the evolution of our deployment strategies was driven by common pain points like scalability, consistency, and reliability, rather than a single service transforming over time. Along the way, I learned that every deployment evolution was driven by actual pain points, not by wanting to use cool new tech.

## Early Days: From Script-based Deployment to Docker
### The Problems That Led to Change
When I joined the team, we weren't using Docker yet. Our deployment process, built on SSH-based scripts, had several pain points:

- **Rollback difficulties**: Reverting to previous versions was complex and error-prone, often taking hours to resolve dependency conflicts.
- **Environment inconsistencies**: "It works on my machine" was a daily reality.
- **Dependency mismatches**: Development and production versions would drift apart.
- **Deployment script management**: Scripts were scattered and hard to maintain.
- **Server management overhead**: Managing deployment servers was becoming unwieldy.

### The Docker Introduction
We moved from SSH-based deployments to using version control for build/deployment scripts, which then handled Docker builds and deployments. This shift wasn't immediate-it was driven by the accumulated pain of our existing process, like constant "dependency hell" where applications worked locally but failed in production.

### Docker Learning Curve
The transition to Docker came with its own challenges:

**Image Size Issues**: Our first Docker images were massive, often exceeding 1GB due to bloated base images and unnecessary dependencies. We optimized by:
- Switching to lightweight Alpine-based images (e.g., from `node:16` to `node:16-alpine`).
- Implementing multi-stage builds to compile dependencies separately.
- Minimizing layers by consolidating RUN commands.
- Using `.dockerignore` to reduce build context.
- Removing unused libraries by auditing and cleaning up `package.json` to eliminate deprecated or unnecessary npm packages.

**Build Inconsistencies**: Sometimes builds that worked fine outside Docker would fail inside containers due to environment differences. This taught me the importance of understanding the containerized environment.

**Network Configuration**: Container networking was a new concept that required dedicated learning. For example, we faced issues with misconfigured certificates and incorrect port mappings that broke inter-container communication (e.g., Nginx containers returning 502 errors due to incorrect SSL certificate references). Thoroughly studying Docker’s official networking documentation and experimenting with bridge networks resolved these problems.

**Resource Management**: Being containerized meant being more careful about setting resource limits and requests to avoid performance issues.

### The Benefits We Gained
Despite the learning curve, Docker solved our major pain points:
- **Easy rollbacks**: Tagged images made version management simple, reducing rollback time from hours to minutes.
- **Environment consistency**: Eliminated the "works on my machine" problem.
- **Dependency isolation**: No more conflicting system-level dependencies.
- **Simplified deployments**: Containerized deployments were more reliable.
- **Quantifiable Wins**: Previously, script-based deployments took 20-30 minutes due to manual dependency setup, and rollbacks could take hours to resolve conflicts. With Docker, deployments dropped to under 5 minutes, and rollbacks became near-instantaneous by reverting to tagged images. While we didn’t track precise CPU or memory metrics, the isolation of containers reduced debugging overhead, saving significant time during incidents.

### The Learning Curve
Docker wasn't just about copying existing Dockerfiles-it required understanding the entire ecosystem:

**Image Optimization Reality Check**: I learned about Docker optimization through real examples:
- Understanding the difference between base images (e.g., Ubuntu vs. Alpine).
- Learning about multi-stage builds.
- Seeing how layer caching worked.
- Using `.dockerignore` to exclude unnecessary files.

**Network Configuration**: This was where I first encountered container networking concepts:
- Service communication between containers.
- Port mapping and exposure.
- Understanding container isolation.

**Optimization Impact**: By adopting Alpine images, cleaning up `package.json`, and using multi-stage builds, we reduced image sizes by over 70% (e.g., from 1.2GB to ~300MB for our API server). This sped up builds and deployments, especially on resource-constrained CI/CD pipelines.

### The Wins
- **Environment consistency**: Containers eliminated "it works on my machine" issues.
- **Dependency isolation**: No more conflicting system-level dependencies.
- **Reproducible builds**: Same image could run anywhere.
- **Version control**: Tagged images made deployment tracking easier.

## Multi-Service Orchestration: Docker Compose Experience
### The Growing Complexity
As I learned more, I saw how applications needed multiple services: databases, caches, load balancers. Managing these services individually was complex, especially as our services grew to support 300+ employees.

### Docker Compose in Action
Working with Docker Compose taught me about service orchestration, including how to define and manage multi-container applications with dependencies like databases and caches.

### Zero-Downtime Deployment Concepts
I learned about blue-green deployment strategies through company guidelines and implementations.

**Blue-Green Deployment Implementation**: Our company standardized on blue-green deployments to achieve zero-downtime updates. We spun up a new environment (green) alongside the existing one (blue), routed traffic to the new environment after validation, and decommissioned the old one. The setup was smooth thanks to clear guidelines, but challenges arose with specific applications.

**The Socket Server Challenge**: When working with a Slack bot (a socket server) that integrated with an external service we didn’t control, we discovered an issue with multiple instances during blue-green transitions-duplicate event processing. To solve this, we implemented event deduplication by assigning each event a unique ID and storing it in our MongoDB database. MongoDB was chosen because the company recommended either MongoDB or MySQL, and MongoDB’s flexible schema was ideal for the Slack bot’s frequently changing data structure. Additionally, the data volume was low (as data was deleted after a set period), and queries were only needed for deployment validation, making MongoDB suitable. Before processing an event, the application checked the database to ensure it hadn’t been handled. This prevented duplicate actions and taught me about idempotency in distributed systems. We considered Redis for faster lookups but stuck with MongoDB to avoid additional infrastructure management overhead, as it was sufficient for our needs.

### The Single Host Reality
Docker Compose worked well for development and small deployments, especially when our tool was used by just a 7-person team. However, it had limitations:
- **Single point of failure**: Everything depended on one host.
- **No automatic failover**: Services couldn’t migrate automatically.
- **Scaling limitations**: Only vertical scaling was possible.

Initially, these limitations weren’t critical due to our small scale. As our services grew to support 300+ employees, a single server outage caused significant productivity losses, making distributed orchestration necessary. This pushed us toward Docker Swarm and eventually Kubernetes.

## The Kubernetes Transition
### Why the Move to Kubernetes?
As our services became critical, we needed to comply with company security policies and integrate with the organization’s infrastructure. Initially, we used Docker Swarm as an interim step because it was simpler and leveraged our Docker expertise, avoiding Kubernetes’ steep learning curve. However, our tools, originally used informally, became subject to company audits as new features elevated their importance. This necessitated integration with the company’s production Kubernetes cluster, which already had established security controls and compliance measures, making Kubernetes the logical choice for scalability and organizational alignment.

### Challenges I Encountered
**Cross-team Collaboration**: Communication was a bigger challenge than the technical aspects. The platform team had information we didn’t, and vice versa. I learned to review their internal wiki for context, clearly articulate our application’s needs, and prepare data-driven arguments (e.g., RPS metrics) to streamline discussions.

### What I Learned
The Kubernetes environment reduced infrastructure management overhead and provided benefits in scalability and security, but it came at the cost of less direct control. Previously, our team managed Swarm servers and deployment infrastructure, allowing hands-on debugging (e.g., direct access to servers via `docker exec`). After transitioning to Kubernetes, managed by another team, we became service developers rather than server administrators, reducing our ability to directly debug infrastructure issues. This trade-off taught me the importance of clear communication with platform teams to address issues effectively.

## Logging and Monitoring Evolution
Our logging and monitoring strategy evolved significantly over the three-year journey, driven by the need for reliability, scalability, and actionable insights. Below is the progression from basic file-based logging to a robust ELK stack implementation.

### Phase 1: Manual File-Based Logging
Initially, we relied on basic file-based logging with log rotation on deployment servers. This approach was simple but had significant drawbacks:
- **Challenges**: Logs were scattered across servers, requiring manual inspection via `grep`, which was time-consuming and error-prone. When containers were removed, logs disappeared, complicating debugging.
- **Solution**: We implemented cron jobs for log rotation and began extracting logs to external storage to ensure persistence.

### Phase 2: Centralized Log Collection
As our services grew, managing logs across multiple containers became unwieldy. We centralized log collection to address this:
- **Implementation**: Initially, each container sent logs to Elasticsearch via individual HTTP POST requests. This caused network usage spikes and occasional log loss due to connectivity issues.
- **Optimization**: We switched to batch processing, aggregating logs into files based on size and uploading them to Elasticsearch via cron jobs. This reduced network overhead and improved reliability.
- **Impact**: Centralized collection enabled faster incident diagnosis, reducing debugging time compared to manual log inspection.

### Phase 3: ELK Stack with Preprocessing
To support 300+ engineers and meet growing monitoring demands, we adopted the ELK stack (Elasticsearch, Logstash, Kibana):
- **Preprocessing**: We used Python-based custom scripts to preprocess logs, extracting performance metrics (e.g., API response times, error rates) and formatting them as structured JSON for Elasticsearch compatibility. This was necessary due to CI and service constraints that prevented source-level formatting.
- **Security**: To comply with data policies, we avoided logging sensitive information, reducing compliance risks.
- **Challenges**: Early on, we avoided tools like Fluentd or Logstash to minimize the learning curve, relying on our Python expertise. As log volume grew, we transitioned to Logstash for streamlined integration with Elasticsearch.
- **Pipeline Optimization**: Initially, our pipeline followed a preprocessing → alert sending → log storage flow. We later adjusted to log storage → data processing & alert sending, improving efficiency by reducing redundant processing.
- **Impact**: Structured logs enabled real-time Kibana dashboards, transforming raw data into actionable insights like error rate trends and API performance metrics.

### Phase 4: Automated Dashboards and Alerting
The final phase focused on proactive monitoring:
- **Dashboards**: We created Kibana dashboards to visualize performance metrics, such as response times and error rates, enabling quick identification of anomalies.
- **Alerting**: We implemented alerts for critical events (e.g., high error rates, slow API responses), reducing incident response times from 30 minutes to under 5 minutes in some cases.
- **Quantifiable Wins**: Centralized logging and dashboards reduced debugging overhead, and batch processing lowered network usage spikes. While we didn’t track precise metrics like log loss rates, reliability improved noticeably, with no reported log losses after optimization.

### Key Lessons
- **Start Simple, Scale Deliberately**: File-based logging sufficed initially, but centralized collection and ELK were critical as scale increased.
- **Avoid Over-Engineering**: Python scripts were sufficient early on; adopting Logstash later aligned with growing complexity.
- **Structure Matters**: Structured JSON logs and batch processing were pivotal for scalability and reliability.
- **Documentation Is Critical**: Lack of early documentation on log preprocessing logic caused delays in troubleshooting, reinforcing the need for detailed records.

This evolution from manual log inspection to automated, centralized monitoring was crucial for maintaining service reliability and supporting a growing user base. It taught me the importance of aligning logging strategies with organizational scale and complexity.


## Key Lessons Learned
### Technical Insights
- **Documentation First**: Official documentation is more reliable than blog posts or quick answers. Take time to understand fundamentals.
- **Right Tool for the Job**: Don’t choose technology just because it’s new. I once thought Kubernetes was ideal for a single-service project, but Docker Compose would’ve sufficed. Consider team capabilities and actual requirements.
- **Gradual Learning**: Every technology change requires learning time. Factor this into planning and expectations.
- **Document Everything**: I regret not documenting more during this journey. Detailed notes would’ve improved this post and helped troubleshoot faster.

### Organizational Insights
- **Communication Matters**: Understand other teams’ perspectives and constraints. For example, when proposing Docker, some team members suggested optimizing scripts. We tested their approach but found it unscalable, and data on deployment failures convinced the team.
- **Knowledge Sharing**: Don’t keep information to yourself. Shared knowledge helps everyone and creates better opportunities.
- **Incremental Changes**: Major infrastructure changes should be gradual, well-tested, and carefully planned.

### Memorable Incidents
- **Script-Based Deployments**: Dependency hell-constant “it works on my machine” issues ate up hours of debugging.
- **Docker**: Networking nightmares, like certificate misconfigurations, taught me to double-check container communication.
- **Docker Compose**: Managing a single host became a bottleneck as our user base grew to 300+ employees.
- **Kubernetes**: Cross-team collaboration challenges taught me the value of clear communication and data-driven arguments.

## Final Thoughts
Looking back at this 3-year journey, I can see how each phase built on the previous one. Every transition was driven by real needs: better consistency, improved scalability, enhanced security, or organizational requirements.

The most valuable thing I learned wasn't mastering any specific technology-it was understanding problems deeply, communicating effectively with teams, and making decisions that serve users and organizational needs.

