---
title: "Building Withsy: Why I Created Yet Another AI Chat Platform (And What I Learned)"
description: "A candid look at building a personal AI chat platform, from solving UI frustrations to achieving 97% performance improvements, and the honest lessons about product-market fit in a crowded AI landscape."
tags: ["project", "withsy", "ai", "chat interface", "product development", "performance optimization"]
date: "2025-05-09"
---

> Wanna try it yourself? [Withsy](https://withsy.chat) is live-feel free to explore it while reading!

## The Itch That Needed Scratching

Like many developers, I found myself using multiple AI chat platforms daily-ChatGPT, Claude, Gemini, Perplexity. Each has its own personality and strengths: ChatGPT feels conversational and personal, Claude provides structured professional responses, Perplexity excels at research. But despite their individual character, small frustrations kept piling up in my daily workflow.

Why couldn't I customize the interface to match my preferences like I do with other tools? Why couldn't I see or control the prompts being applied to my conversations? Why was there no good way to organize and save important messages across different models?

So I did what any developer would do: I built my own AI chat platform called **Withsy**.

## The Reality Check

Here's the uncomfortable truth I discovered after building Withsy: **it solves problems that matter to me, but may not provide enough value for most users to switch**. With ChatGPT, Claude, Gemini, and Perplexity all offering generous free tiers and distinct user experiences, convincing someone to try yet another AI platform is a tough sell.

But the journey taught me valuable lessons about user experience, performance optimization, and product-market fit that I want to share.

## Core Features: Solving Real Frustrations

### 1. Unified Personalization Layer

**The Problem**: While each AI platform has its own personality, none offered the level of interface customization I wanted for my daily workflow.

**The Solution**: 
- **Custom themes** that go beyond what existing platforms offer. While ChatGPT has its distinctive dark interface and Claude maintains its professional aesthetic, I wanted something that could adapt to my mood and workflow - blue themes for focused work sessions, warmer tones for creative projects
- **Custom model nicknames** instead of technical identifiers. Rather than seeing "gemini-2.0-flash-exp" or "claude-3-5-sonnet-20241022", I could use friendly names like "gem" or "claude" that felt more natural in conversation
- **Unified visual identity** across different AI models, creating consistency while preserving each model's unique response style

This wasn't about replacing the distinct personalities of different AI models, but about creating a consistent interface layer that made switching between them seamless.

### 2. Complete Prompt Transparency

**The Problem**: Hidden system prompts that make AI responses unpredictable and sometimes frustratingly filtered.

**The Solution**: Full visibility and control over prompts applied to conversations.

Users can:
- Set default prompts for all chats
- Create and manage prompt templates for specific use cases
- See exactly what prompts are being applied (no hidden wrappers)

This isn't just about control-it's about understanding. When platforms apply invisible prompts "for personalization" or safety, users can unknowingly receive biased information without understanding why. Transparency helps users understand the context behind responses.

### 3. Conversation Branching

**The Problem**: When an AI provides three different approaches to a problem, asking follow-up questions about each one pollutes the context and leads to confused responses.

**The Solution**: Branch functionality that lets users fork conversations at any point.

Instead of creating separate chats and losing track of the original context, users can explore different conversation paths while maintaining clear relationships between discussions.

**Technical Implementation**: I separated the data model into `chat` and `branch` entities. Chats maintain lists of their branches, branches reference their origin chat, and the API serves only the relevant message context up to the branch point.

### 4. Cross-Platform Message Management

**The Problem**: Great conversations and insights scattered across different AI platforms with no good way to organize or reference them later. Plus, simple workflow frustrations like having to manually retype questions when switching between platforms or starting fresh conversations.

**The Solution**: 
- Star/bookmark system to pin important chats across all models
- Message saving functionality (no more copying to separate note apps)
- **One-click question copying** - because retyping the same question across different AI platforms or when resetting context gets old fast
- Unified search across conversations with different AI models
- Clean organization that preserves the context of which model provided which insights

## The Technical Deep Dive

### Performance Optimization: From 2.8s to 83ms

Like most developers who've used AI chat platforms extensively, I'd noticed how they tend to get sluggish as conversations grow longer. When I built Withsy, I initially ran into the same issue - and admittedly, my initial code was pretty messy, which made the problem worse.

**The Issue**: Loading chats with extensive message history would freeze the interface, even after page refresh. Chrome DevTools Performance tab revealed massive re-rendering cycles with unnecessary component updates and synchronous markdown parsing for every message.

Here's how I tackled it:

**1. Smart Component Memoization**
```typescript
const ChatBubble = memo(ChatBubbleComponent, (prevProps, nextProps) => {
  return prevProps.id === nextProps.id &&
         prevProps.isBookmarked === nextProps.isBookmarked &&
         prevProps.text === nextProps.text &&
         prevProps.status === nextProps.status;
});
```

**2. Optimized DOM References**
```typescript
// Efficient message scrolling without re-rendering everything
<div ref={(el) => { messageRefs.current[msg.id] = el }}>
```

**3. Conditional Rendering**
- Messages over 150 characters are collapsed by default
- Reasoning text only renders when requested
- Smart auto-scroll that respects user's scroll position

**4. Lazy Loading Strategy**
Only render visible content and reasoning text on demand.

**Result**: Chat loading time dropped from 2.8 seconds to 83ms-a 97% improvement.

### Multi-Model Integration

Multi-model support within single conversations was one of the core features I wanted from the start. Users could ask Grok a question, then get Gemini's perspective on the same topic without losing context or switching platforms.

Most AI SDKs follow OpenAI's standard, making integration straightforward (except Google's Gemini, which required their specific SDK). The ecosystem is consolidating nicely around common interfaces.

## Tech Stack Choices

**Next.js Full-Stack**: Not my preferred architecture (I find Next.js backend debugging frustrating), but budget constraints meant single-server deployment. As always, **infrastructure decisions come down to money**.

**PostgreSQL**: Reliable choice for the relational data patterns (chats, branches, users, messages).

**Why Next.js for Frontend**: Despite my backend reservations, Next.js remains my go-to for complex, app-like interfaces. Development speed was crucial for this side project.

## Lessons Learned

### User-Driven Development Actually Works
I tested features incrementally with family and friends across different age groups. This wasn't formal user research, but getting feedback on minimal UI implementations as I built each feature proved invaluable.

**Early wins**: The prompt transparency and message saving features resonated immediately with users who'd experienced similar frustrations across multiple AI platforms.

**Quick pivots**: Thread and tab functionality got negative feedback fast-users found them confusing and unnecessary when branch and star systems already covered their needs.

### Feature Creep is Real (And User Feedback Saves You)
I initially built thread functionality (thinking it would be useful like Slack threads) and tab organization (like Notion). Both got deprecated because:
- Users actively disliked the added complexity
- Branch functionality covered thread use cases more intuitively  
- Star system made tabs redundant
- Even I stopped using them after the novelty wore off

**Lesson**: Listen to users, but also pay attention to your own usage patterns. If you don't reach for a feature you built, others probably won't either.

### The Product-Market Fit Reality
Building something because "wouldn't this be nice?" isn't enough. Coming from corporate development where every feature solves a specific problem, I realized personal projects need the same problem-solving focus to gain traction.

The feedback from friends and family confirmed something important: while they appreciated individual features, most didn't feel compelled to switch from their current AI platforms. However, a few users did connect with specific aspects - they found the custom character avatars endearing, enjoyed the interface customization, and really valued the message bookmarking functionality. I've also been using Withsy as my primary AI platform since building it. The switching cost (learning new UI, losing chat history, disrupting established workflows) outweighed the benefits for casual users.

**Who might actually use Withsy?**
- Power users who work across multiple AI models daily
- Users who want to save and organize insights from AI conversations  
- People who demand prompt transparency and control

But here's the real challenge: most potential users are already comfortable with existing platforms' free tiers. Why would someone pay for or learn a new platform when ChatGPT, Claude, and others already meet their needs? 

Plus, these established platforms offer features that Withsy doesn't yet have - file attachments, advanced editors, image generation, and other capabilities that users have come to expect. The switching cost outweighs the benefits for most users when they'd be giving up functionality they're used to.

## The Honest Assessment

Withsy scratches my personal itches perfectly, but that's not enough for a sustainable product. The major AI platforms each offer distinct value propositions and generous free tiers. Unless you're solving a problem that keeps users awake at night, "nice to have" features won't drive adoption from established platforms.

That said, building Withsy was incredibly valuable:
- **Performance optimization skills** from tackling complex rendering challenges
- **Product thinking** about feature necessity vs. feature creep  
- **Technical exploration** of the current AI ecosystem and integration patterns
- **Honest evaluation** of personal preferences vs. market needs

## What's Next

Withsy remains my personal AI workspace of choice, and I'll continue developing it step by step in the direction I want. There's still a lot to build - file attachments, better editors, and other features that users expect from modern AI platforms.

The plan isn't to compete with established platforms immediately, but to gradually build toward a more complete solution while learning from each iteration.

For developers considering similar projects: build for the learning, not just the outcome. Sometimes the most valuable product insights come from products that don't quite make it to market, but teach you about the gap between personal preferences and broader user needs.